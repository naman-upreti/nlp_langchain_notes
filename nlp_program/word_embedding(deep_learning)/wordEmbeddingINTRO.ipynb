{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876ea566-97b8-410f-bd7d-0b033aa582d1",
   "metadata": {},
   "source": [
    "## üî• **Word Embedding: Count-Based vs. Deep Learning-Based Approaches**  \n",
    "\n",
    "Word embedding is a technique used to represent words in numerical form (vectors), capturing their meaning and relationships in a high-dimensional space. There are two main types of word embedding approaches:  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå **1Ô∏è‚É£ Count-Based Word Embeddings**  \n",
    "Count-based methods use **co-occurrence** statistics of words in a large corpus. These methods rely on **word frequency and distribution** to create embeddings.  \n",
    "\n",
    "### üîπ **Examples of Count-Based Embeddings:**  \n",
    "1. **Bag-of-Words (BoW)**  \n",
    "   - Converts text into a matrix of word occurrences (word counts).  \n",
    "   - Ignores word order and meaning.  \n",
    "\n",
    "2. **Term Frequency - Inverse Document Frequency (TF-IDF)**  \n",
    "   - Measures how important a word is in a document relative to a collection of documents.  \n",
    "   - More frequent words are downweighted to balance importance.  \n",
    "\n",
    "3. **Co-Occurrence Matrix (Word Context Matrix)**  \n",
    "   - Uses word co-occurrence in a fixed window size to create a word-word matrix.  \n",
    "   - High-dimensional representation (sparse).  \n",
    "\n",
    "4. **Latent Semantic Analysis (LSA)**  \n",
    "   - Reduces high-dimensional word vectors using **Singular Value Decomposition (SVD)**.  \n",
    "   - Captures hidden relationships between words.  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå **2Ô∏è‚É£ Deep Learning-Based Word Embeddings**  \n",
    "Deep learning-based embeddings are trained using **neural networks** to learn contextual relationships between words in large datasets. These embeddings capture **semantic meaning** more effectively than count-based methods.  \n",
    "\n",
    "### üîπ **Examples of Deep Learning-Based Embeddings:**  \n",
    "1. **Word2Vec (Skip-Gram & CBOW)**  \n",
    "   - Trained using a shallow neural network.  \n",
    "   - **CBOW** (Continuous Bag of Words): Predicts a word based on its surrounding words.  \n",
    "   - **Skip-Gram**: Predicts surrounding words based on a given word.  \n",
    "\n",
    "2. **GloVe (Global Vectors for Word Representation)**  \n",
    "   - Combines co-occurrence matrix and neural networks for word embedding.  \n",
    "   - Learns relationships between words using matrix factorization.  \n",
    "\n",
    "3. **FastText (Word2Vec with Subword Information)**  \n",
    "   - Builds word representations using subwords (character n-grams).  \n",
    "   - Helps in handling out-of-vocabulary (OOV) words.  \n",
    "\n",
    "4. **Transformer-Based Embeddings (BERT, GPT, etc.)**  \n",
    "   - Contextual word embeddings learned using deep Transformer networks.  \n",
    "   - **BERT** (Bidirectional Encoder Representations from Transformers): Generates embeddings considering both left and right context.  \n",
    "   - **GPT** (Generative Pretrained Transformer): Focuses on generating text using sequential processing.  \n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Comparison: Count-Based vs. Deep Learning-Based Word Embeddings**  \n",
    "\n",
    "| Feature | Count-Based (BoW, TF-IDF) | Deep Learning-Based (Word2Vec, BERT) |\n",
    "|---------|---------------------------|-------------------------------------|\n",
    "| **Training Complexity** | Low (Matrix operations) | High (Neural Networks) |\n",
    "| **Word Order Awareness** | No | Yes (Contextual embeddings) |\n",
    "| **Dimensionality** | High (Sparse Vectors) | Low (Dense Vectors) |\n",
    "| **Handling Out-of-Vocabulary (OOV) Words** | Poor | Better (FastText, BERT) |\n",
    "| **Semantic Understanding** | Limited | Strong |\n",
    "| **Computational Cost** | Low | High |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **How to Train a Deep Learning Word Embedding Model?**  \n",
    "You can train your own word embeddings using deep learning frameworks like **TensorFlow** or **PyTorch**.  \n",
    "\n",
    "### üîπ **Steps to Train a Word2Vec Model using Gensim**  \n",
    "```python\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\"I love deep learning\", \"Word embeddings capture meaning\", \"Natural Language Processing is amazing\"]\n",
    "\n",
    "# Tokenizing sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Training Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get vector representation of a word\n",
    "word_vector = model.wv[\"learning\"]\n",
    "print(word_vector)  # Prints 100-dimensional vector\n",
    "```\n",
    "\n",
    "### üîπ **Using Pretrained Word Embeddings (GloVe in TensorFlow)**  \n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load pretrained GloVe embeddings\n",
    "embedding_dict = {}\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_dict[word] = vector\n",
    "\n",
    "# Convert word to vector\n",
    "print(embedding_dict[\"deep\"])  # Prints GloVe vector for \"deep\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **Conclusion**  \n",
    "- **Count-based** embeddings are simple and interpretable but **lack semantic meaning**.  \n",
    "- **Deep learning-based** embeddings capture **context and meaning** but require more computation.  \n",
    "- **Transformer-based models (BERT, GPT)** provide **context-aware** representations and are widely used in modern NLP.  \n",
    "\n",
    "üîπ **Which one should you use?**  \n",
    "- Use **TF-IDF** for small datasets and traditional ML models.  \n",
    "- Use **Word2Vec/GloVe** for general-purpose word embeddings.  \n",
    "- Use **BERT/GPT** for advanced **context-aware** NLP tasks.  \n",
    "\n",
    "Would you like an implementation with **Streamlit** or **Dash** to visualize word embeddings? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510ff74-9328-44f4-b405-1a6835e6c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
