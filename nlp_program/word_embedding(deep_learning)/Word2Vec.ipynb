{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8ad2b9-f4ce-4744-be86-d7e2dc2912ef",
   "metadata": {},
   "source": [
    "Here are short notes on **Word2Vec** \n",
    "\n",
    "---\n",
    "\n",
    "```markdown\n",
    "# üìå Word2Vec - Quick Notes\n",
    "\n",
    "## üîπ What is Word2Vec?\n",
    "Word2Vec is a deep learning-based word embedding technique that transforms words into vector representations, capturing their semantic meaning.\n",
    "\n",
    "## üîπ Types of Word2Vec Models\n",
    "1. **CBOW (Continuous Bag of Words)**  \n",
    "   - Predicts a target word based on its surrounding context words.  \n",
    "   - Faster and works well for small datasets.  \n",
    "\n",
    "2. **Skip-Gram**  \n",
    "   - Predicts surrounding words based on a given target word.  \n",
    "   - Works better for rare words in large datasets.  \n",
    "\n",
    "## üîπ How Word2Vec Works?\n",
    "- Uses a **shallow neural network** with one hidden layer.  \n",
    "- Converts words into **dense vectors** based on co-occurrence relationships.  \n",
    "- Word vectors are learned by optimizing the network through **backpropagation**.  \n",
    "\n",
    "## üîπ Key Parameters\n",
    "- `vector_size`: Number of dimensions in word embeddings.  \n",
    "- `window`: Number of words considered around a target word.  \n",
    "- `min_count`: Ignores words that appear less than this threshold.  \n",
    "- `workers`: Number of CPU threads used in training.  \n",
    "\n",
    "## üîπ Example: Training a Word2Vec Model in Python\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = [\"Word embeddings capture meaning\", \"NLP is amazing\", \"Deep learning improves AI\"]\n",
    "tokenized_sentences = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get word vector\n",
    "print(model.wv[\"learning\"])\n",
    "```\n",
    "\n",
    "## üîπ Advantages of Word2Vec\n",
    "‚úÖ Captures semantic relationships (e.g., \"king - man + woman = queen\")  \n",
    "‚úÖ Produces dense, meaningful vector representations  \n",
    "‚úÖ Efficient compared to traditional word embeddings  \n",
    "\n",
    "## üîπ Limitations\n",
    "‚ùå Struggles with **out-of-vocabulary (OOV) words**  \n",
    "‚ùå Ignores **word order and sentence structure**  \n",
    "‚ùå Not **context-aware** (unlike BERT/GPT)  \n",
    "\n",
    "## üîπ Applications\n",
    "- Sentiment Analysis  \n",
    "- Machine Translation  \n",
    "- Chatbots & Virtual Assistants  \n",
    "- Document Clustering  \n",
    "\n",
    "üîπ **Alternatives**: FastText (handles OOV words), GloVe (combines count-based + neural embeddings), BERT (contextual embeddings).  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8c27f6-aeb3-42e3-89b7-c0fc8029fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\naman\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\naman\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\naman\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\naman\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c6338b-e2d4-49c4-ab46-b52138afd1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'word2vec_data.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "text = {\n",
    "    \"Sentence_ID\": [1, 2, 3, 4, 5],\n",
    "    \"Text\": [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The dog barked at the stranger\",\n",
    "        \"A man walked into the park with his dog\",\n",
    "        \"She enjoys reading books in the library\",\n",
    "        \"The sun is shining brightly in the sky\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(text)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"word2vec_data.csv\", index=False)\n",
    "\n",
    "print(\"CSV file 'word2vec_data.csv' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e37202-3deb-4566-ab0c-370330a76698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"word2vec_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c3cc9b-c14e-47a4-a46a-f26f61710925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The cat sat on the mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The dog barked at the stranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A man walked into the park with his dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>She enjoys reading books in the library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The sun is shining brightly in the sky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_ID                                     Text\n",
       "0            1                   The cat sat on the mat\n",
       "1            2           The dog barked at the stranger\n",
       "2            3  A man walked into the park with his dog\n",
       "3            4  She enjoys reading books in the library\n",
       "4            5   The sun is shining brightly in the sky"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4e72d8-c1cc-4ba0-968b-9063a9a79bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence_ID', 'Text'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebf3f8-fb1c-4203-b83c-71462f9e6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed9ce044-fc56-40be-9d9a-3b184cce0590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Text  \\\n",
      "0                   The cat sat on the mat   \n",
      "1           The dog barked at the stranger   \n",
      "2  A man walked into the park with his dog   \n",
      "3  She enjoys reading books in the library   \n",
      "4   The sun is shining brightly in the sky   \n",
      "\n",
      "                                      Tokenized_Text  \n",
      "0                      [the, cat, sat, on, the, mat]  \n",
      "1              [the, dog, barked, at, the, stranger]  \n",
      "2  [a, man, walked, into, the, park, with, his, dog]  \n",
      "3    [she, enjoys, reading, books, in, the, library]  \n",
      "4    [the, sun, is, shining, brightly, in, the, sky]  \n"
     ]
    }
   ],
   "source": [
    "df[\"Tokenized_Text\"] = df[\"Text\"].apply(lambda x : word_tokenize(x.lower()))\n",
    "print(df[[\"Text\",\"Tokenized_Text\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0298455b-4c9e-4d82-ab0a-cb6166e22593",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = df[\"Tokenized_Text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32df9d0e-d066-4b7d-95b3-afdef736ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(sentences = tokenized_sentences , vector_size = 100, window = 5 , min_count = 1 , workers = 1)\n",
    "\n",
    "\n",
    "w2v.save(\"w2v.model\")\n",
    "print(\"Word2Vec model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eea4906-620c-4fba-b882-c30227489f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"w2v.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c110efd3-0337-455f-b01e-75bba03bb28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat': \n",
      " [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    }
   ],
   "source": [
    "word_vector = w2v.wv[\"cat\"]\n",
    "print(\"Vector for 'cat': \\n\",word_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d788746-f41e-4104-863d-67bcc1e3b095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barked', 0.19900402426719666), ('a', 0.1727856993675232), ('cat', 0.17032110691070557), ('she', 0.1528787463903427), ('reading', 0.1485336273908615), ('on', 0.14597296714782715), ('sat', 0.06404566019773483), ('park', 0.05354084074497223), ('man', 0.047006841748952866), ('shining', 0.013658874668180943)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.most_similar(\"dog\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab2b16-fe23-4a56-8c38-973e7005b367",
   "metadata": {},
   "source": [
    "üîπ Step 7: Find Similarity Between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "648695b9-2a46-4f4f-8570-37f1cfbe7dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'dog' and 'cat': 0.1703211\n"
     ]
    }
   ],
   "source": [
    "similarity = w2v.wv.similarity(\"dog\", \"cat\")\n",
    "print(\"Similarity between 'dog' and 'cat':\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4a3e7-c190-4438-b6cc-e21d63b5c524",
   "metadata": {},
   "source": [
    "### **Explanation of Word2Vec Model Parameters**\n",
    "```python\n",
    "w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=1)\n",
    "```\n",
    "This line initializes and trains a **Word2Vec** model using `gensim`. Let's break down each parameter:\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ `sentences=tokenized_sentences`**\n",
    "- **What it does**: \n",
    "  - Takes **preprocessed tokenized sentences** as input.\n",
    "  - Each sentence is a **list of words** (tokens).\n",
    "  \n",
    "- **Example:**\n",
    "  ```python\n",
    "  tokenized_sentences = [\n",
    "      ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "      ['the', 'dog', 'barked', 'at', 'the', 'stranger']\n",
    "  ]\n",
    "  ```\n",
    "  Here, `Word2Vec` learns relationships between words based on their **context in these sentences**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ `vector_size=100`**\n",
    "- **What it does**:\n",
    "  - Sets the **number of dimensions** of the word embeddings.\n",
    "  - Each word is represented as a **100-dimensional vector**.\n",
    "\n",
    "- **Why 100?**\n",
    "  - A higher dimension captures more **semantic meaning**.\n",
    "  - Too large = computationally expensive.\n",
    "  - Too small = may lose important relationships.\n",
    "  - Common values: **50, 100, 200, 300**.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  word_vector = w2v.wv[\"cat\"]\n",
    "  print(word_vector.shape)  # Output: (100,)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ `window=5`**\n",
    "- **What it does**:\n",
    "  - Defines the **maximum distance** between the target word and neighboring words.\n",
    "  - If `window=5`, Word2Vec considers **5 words before and after** the target word for training.\n",
    "\n",
    "- **Example:**\n",
    "  ```\n",
    "  Sentence:  \"The dog barked at the stranger\"\n",
    "  \n",
    "  For \"barked\", the context words (window=5) are:\n",
    "  ['The', 'dog', 'at', 'the', 'stranger']\n",
    "  ```\n",
    "\n",
    "- **Smaller `window` (e.g., 2) = More focused relationships**.  \n",
    "- **Larger `window` (e.g., 10) = More generalized meaning**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ `min_count=1`**\n",
    "- **What it does**:\n",
    "  - Ignores words that appear **less than `min_count` times** in the dataset.\n",
    "  - If `min_count=1`, **all words** are considered.\n",
    "  - If `min_count=5`, only words appearing **at least 5 times** are included.\n",
    "\n",
    "- **Why use `min_count`?**\n",
    "  - **Filters rare words** that may be **noise**.\n",
    "  - Improves **training efficiency**.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  w2v = Word2Vec(sentences=tokenized_sentences, min_count=5)\n",
    "  ```\n",
    "  - Here, words appearing **less than 5 times** are ignored.\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ `workers=1`**\n",
    "- **What it does**:\n",
    "  - Defines the **number of CPU threads** used for training.\n",
    "  - If `workers=4`, it uses **4 CPU cores** to speed up training.\n",
    "\n",
    "- **Why use multiple workers?**\n",
    "  - Speeds up training on large datasets.\n",
    "  - On small datasets, `workers=1` is fine.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  import multiprocessing\n",
    "  num_workers = multiprocessing.cpu_count()  # Get number of CPU cores\n",
    "  w2v = Word2Vec(sentences=tokenized_sentences, workers=num_workers)\n",
    "  ```\n",
    "  - This automatically sets `workers` to **use all available CPU cores**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Word2Vec Model Parameters\n",
    "\n",
    "### üîπ `sentences=tokenized_sentences`\n",
    "- Input tokenized sentences (list of words).\n",
    "- Helps the model learn **word relationships**.\n",
    "\n",
    "### üîπ `vector_size=100`\n",
    "- Defines **word embedding dimensions**.\n",
    "- More dimensions = better meaning capture.\n",
    "- Common values: **50, 100, 200, 300**.\n",
    "\n",
    "### üîπ `window=5`\n",
    "- Determines how many words before & after the target word are considered.\n",
    "- **Smaller `window`** = focused meaning.  \n",
    "- **Larger `window`** = generalized meaning.\n",
    "\n",
    "### üîπ `min_count=1`\n",
    "- Filters out words appearing **less than `min_count` times**.\n",
    "- **Higher `min_count`** = removes rare words, speeds up training.\n",
    "\n",
    "### üîπ `workers=1`\n",
    "- Number of CPU cores used for training.\n",
    "- **More workers** = faster training.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Example**\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenized text data\n",
    "tokenized_sentences = [\n",
    "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "    ['the', 'dog', 'barked', 'at', 'the', 'stranger']\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save model\n",
    "w2v.save(\"word2vec_model.model\")\n",
    "\n",
    "# Get vector of 'cat'\n",
    "print(w2v.wv[\"cat\"])\n",
    "```\n",
    "- ‚úÖ **Trains Word2Vec** on example sentences.\n",
    "- ‚úÖ **Saves the trained model**.\n",
    "- ‚úÖ **Retrieves vector representation** of \"cat\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7b793-3618-4043-b47a-4a2b69bc567a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
